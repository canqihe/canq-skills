#!/usr/bin/env python3
"""
AI Data Analyst Skill - Core Analysis Engine (GLMç‰ˆæœ¬)
æ”¯æŒäº¤äº’å¼æ•°æ®åˆ†æï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æŸ¥è¯¢ CSV/Excel æ–‡ä»¶
ä½¿ç”¨æ™ºè°± GLM-4 æ¨¡å‹
"""

import os
import sys
import json
import pickle
import tempfile
import csv
from pathlib import Path
from datetime import datetime

import pandas as pd
import numpy as np
import duckdb
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
import seaborn as sns
from zhipuai import ZhipuAI

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ä¼šè¯å’Œè¾“å‡ºç›®å½•
SESSION_DIR = Path.home() / '.claude' / 'skills' / 'data-analyst' / 'session'
OUTPUT_DIR = Path.home() / 'data_analysis_output'
SESSION_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

SESSION_FILE = SESSION_DIR / 'current_session.pkl'


def preprocess_file(file_path):
    """é¢„å¤„ç†æ•°æ®æ–‡ä»¶"""
    try:
        file_path = Path(file_path)
        if not file_path.exists():
            return None, None, None, f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"

        # è¯»å–æ–‡ä»¶
        if file_path.suffix.lower() == '.csv':
            df = pd.read_csv(file_path, encoding='utf-8', na_values=['NA', 'N/A', 'missing'])
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            df = pd.read_excel(file_path, na_values=['NA', 'N/A', 'missing'])
        else:
            return None, None, None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV æˆ– Excel æ–‡ä»¶"

        # å­—ç¬¦ä¸²åˆ—å¤„ç†
        for col in df.select_dtypes(include=['object']):
            df[col] = df[col].astype(str).replace({r'"': '""'}, regex=True)

        # æ—¥æœŸå’Œæ•°å€¼åˆ—å¤„ç†
        for col in df.columns:
            if 'date' in col.lower():
                df[col] = pd.to_datetime(df[col], errors='coerce')
            elif df[col].dtype == 'object':
                try:
                    df[col] = pd.to_numeric(df[col])
                except (ValueError, TypeError):
                    pass

        # ä¿å­˜ä¸ºä¸´æ—¶ CSV
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as temp_file:
            temp_path = temp_file.name
            df.to_csv(temp_path, index=False, quoting=csv.QUOTE_ALL)

        return temp_path, df.columns.tolist(), df, None
    except Exception as e:
        return None, None, None, f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}"


def save_session(temp_path, columns, df_info, api_key):
    """ä¿å­˜ä¼šè¯çŠ¶æ€"""
    session_data = {
        'temp_path': temp_path,
        'columns': columns,
        'df_info': {
            'shape': df_info.shape,
            'dtypes': df_info.dtypes.astype(str).to_dict(),
            'head': df_info.head(3).to_dict(),
            'description': df_info.describe().to_dict() if len(df_info.describe()) > 0 else {}
        },
        'created_at': datetime.now().isoformat()
    }
    with open(SESSION_FILE, 'wb') as f:
        pickle.dump(session_data, f)
    return session_data


def load_session():
    """åŠ è½½ä¼šè¯çŠ¶æ€"""
    if SESSION_FILE.exists():
        with open(SESSION_FILE, 'rb') as f:
            return pickle.load(f)
    return None


def get_data_schema(temp_path, columns):
    """è·å–æ•°æ®ç»“æ„ä¿¡æ¯ï¼Œç”¨äºç”Ÿæˆ SQL"""
    try:
        conn = duckdb.connect()
        conn.execute(f"CREATE TABLE uploaded_data AS SELECT * FROM read_csv('{temp_path}')")

        # è·å–è¡¨ç»“æ„
        schema_info = conn.execute("DESCRIBE uploaded_data").fetchall()

        # è·å–ç¤ºä¾‹æ•°æ®
        sample_data = conn.execute("SELECT * FROM uploaded_data LIMIT 3").fetchdf()

        conn.close()

        schema_desc = "æ•°æ®è¡¨ uploaded_data çš„ç»“æ„ï¼š\n"
        schema_desc += f"åˆ—å: {', '.join(columns)}\n\n"
        schema_desc += "å­—æ®µè¯¦æƒ…:\n"
        for col in schema_info:
            schema_desc += f"  - {col[0]} ({col[1]})\n"

        schema_desc += f"\nç¤ºä¾‹æ•°æ®ï¼ˆå‰3è¡Œï¼‰:\n{sample_data.to_string()}\n"

        return schema_desc
    except Exception as e:
        return f"è·å–æ•°æ®ç»“æ„å¤±è´¥: {e}"


def generate_sql_with_glm(query, data_schema, api_key):
    """ä½¿ç”¨ GLM ç”Ÿæˆ SQL æŸ¥è¯¢"""
    try:
        client = ZhipuAI(api_key=api_key)

        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º DuckDB SQL æŸ¥è¯¢ã€‚

{data_schema}

é‡è¦è§„åˆ™ï¼š
1. è¡¨åå›ºå®šä¸º 'uploaded_data'
2. åªè¿”å› SQL æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦ä»»ä½•è§£é‡Š
3. ä½¿ç”¨ DuckDB è¯­æ³•
4. å¦‚æœæ¶‰åŠèšåˆï¼Œä½¿ç”¨æ¸…æ™°çš„åˆ—å
5. æ—¥æœŸæ ¼å¼ä½¿ç”¨ ISO 8601

ç¤ºä¾‹ï¼š
ç”¨æˆ·: "é”€å”®é¢æœ€é«˜çš„5ä¸ªäº§å“"
SQL: SELECT product_name, SUM(sales) as total_sales FROM uploaded_data GROUP BY product_name ORDER BY total_sales DESC LIMIT 5;

ç”¨æˆ·: "æŒ‰åœ°åŒºç»Ÿè®¡å¹³å‡é”€å”®é¢"
SQL: SELECT region, AVG(sales) as avg_sales FROM uploaded_data GROUP BY region;
"""

        response = client.chat.completions.create(
            model="glm-4-flash",  # ä½¿ç”¨ GLM-4-Flash (å¿«é€Ÿä¸”ç»æµ)
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·ä¸ºä»¥ä¸‹é—®é¢˜ç”Ÿæˆ SQL: {query}"}
            ],
            temperature=0.1,
            max_tokens=500
        )

        sql_query = response.choices[0].message.content.strip()

        # æ¸…ç†å¯èƒ½çš„ markdown æ ‡è®°
        if sql_query.startswith("```sql"):
            sql_query = sql_query.replace("```sql", "").replace("```", "").strip()
        elif sql_query.startswith("```"):
            sql_query = sql_query.replace("```", "").strip()

        return sql_query, None
    except Exception as e:
        return None, f"GLM API è°ƒç”¨å¤±è´¥: {e}"


def execute_sql(sql_query, temp_path):
    """æ‰§è¡Œ SQL æŸ¥è¯¢"""
    try:
        conn = duckdb.connect()
        conn.execute(f"CREATE OR REPLACE VIEW uploaded_data AS SELECT * FROM read_csv('{temp_path}')")

        result_df = conn.execute(sql_query).fetchdf()
        conn.close()

        return result_df, None
    except Exception as e:
        return None, f"SQL æ‰§è¡Œå¤±è´¥: {e}"


def interpret_results_with_glm(query, result_df, api_key):
    """ä½¿ç”¨ GLM è§£è¯»æŸ¥è¯¢ç»“æœ"""
    try:
        client = ZhipuAI(api_key=api_key)

        result_text = f"æŸ¥è¯¢ç»“æœ:\n{result_df.to_string(index=False)}\n\n"
        result_text += f"å…± {len(result_df)} è¡Œæ•°æ®"

        response = client.chat.completions.create(
            model="glm-4-flash",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æå¸ˆï¼Œç”¨ç®€æ´æ˜“æ‡‚çš„ä¸­æ–‡è§£è¯»æ•°æ®æŸ¥è¯¢ç»“æœã€‚"},
                {"role": "user", "content": f"ç”¨æˆ·é—®é¢˜: {query}\n\n{result_text}\n\nè¯·ç”¨ä¸­æ–‡è§£è¯»è¿™ä¸ªç»“æœã€‚"}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        interpretation = response.choices[0].message.content
        return interpretation
    except Exception as e:
        return f"ç»“æœè§£è¯»å¤±è´¥: {e}"


def create_visualization(result_df, query):
    """æ ¹æ®æŸ¥è¯¢ç»“æœç”Ÿæˆå¯è§†åŒ–"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    charts_created = []

    try:
        # æ£€æŸ¥æ•°æ®æ˜¯å¦é€‚åˆå¯è§†åŒ–
        if len(result_df) == 0:
            return charts_created

        # ç¡®å®šå›¾è¡¨ç±»å‹
        numeric_cols = result_df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = result_df.select_dtypes(include=['object']).columns.tolist()

        # åœºæ™¯1: æœ‰åˆ†ç»„å’Œæ•°å€¼ï¼ˆæŸ±çŠ¶å›¾ï¼‰
        if len(categorical_cols) >= 1 and len(numeric_cols) >= 1:
            fig, ax = plt.subplots(figsize=(12, 6))

            cat_col = categorical_cols[0]
            num_col = numeric_cols[0]

            # é™åˆ¶æ˜¾ç¤ºæ•°é‡
            if len(result_df) > 20:
                plot_df = result_df.head(20)
                title_suffix = f"ï¼ˆå‰20åï¼Œå…±{len(result_df)}æ¡ï¼‰"
            else:
                plot_df = result_df
                title_suffix = ""

            plot_df.plot(kind='bar', x=cat_col, y=num_col, ax=ax, color='steelblue')
            ax.set_xlabel(cat_col, fontsize=12)
            ax.set_ylabel(num_col, fontsize=12)
            ax.set_title(f'{query}', fontsize=14, fontweight='bold')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()

            chart_path = OUTPUT_DIR / f'bar_chart_{timestamp}.png'
            plt.savefig(chart_path, dpi=150, bbox_inches='tight')
            plt.close()
            charts_created.append(str(chart_path))

        # åœºæ™¯2: æ—¶é—´åºåˆ—ï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        elif any('date' in col.lower() or 'time' in col.lower() or 'æ—¥æœŸ' in col or 'æ—¶é—´' in col
                 for col in result_df.columns):
            date_col = None
            for col in result_df.columns:
                if 'date' in col.lower() or 'time' in col.lower() or 'æ—¥æœŸ' in col or 'æ—¶é—´' in col:
                    date_col = col
                    break

            if date_col and len(numeric_cols) > 0:
                fig, ax = plt.subplots(figsize=(12, 6))
                num_col = numeric_cols[0]

                result_df_sorted = result_df.sort_values(date_col)
                ax.plot(result_df_sorted[date_col], result_df_sorted[num_col],
                       marker='o', linewidth=2, markersize=6)
                ax.set_xlabel(date_col, fontsize=12)
                ax.set_ylabel(num_col, fontsize=12)
                ax.set_title(f'{query}', fontsize=14, fontweight='bold')
                plt.xticks(rotation=45, ha='right')
                plt.grid(True, alpha=0.3)
                plt.tight_layout()

                chart_path = OUTPUT_DIR / f'line_chart_{timestamp}.png'
                plt.savefig(chart_path, dpi=150, bbox_inches='tight')
                plt.close()
                charts_created.append(str(chart_path))

        # åœºæ™¯3: åªæœ‰æ•°å€¼åˆ—ï¼ˆç›´æ–¹å›¾ï¼‰
        elif len(numeric_cols) >= 1:
            fig, ax = plt.subplots(figsize=(10, 6))
            num_col = numeric_cols[0]

            ax.hist(result_df[num_col].dropna(), bins=20, color='steelblue', edgecolor='black', alpha=0.7)
            ax.set_xlabel(num_col, fontsize=12)
            ax.set_ylabel('é¢‘æ•°', fontsize=12)
            ax.set_title(f'{query} - åˆ†å¸ƒ', fontsize=14, fontweight='bold')
            plt.tight_layout()

            chart_path = OUTPUT_DIR / f'histogram_{timestamp}.png'
            plt.savefig(chart_path, dpi=150, bbox_inches='tight')
            plt.close()
            charts_created.append(str(chart_path))

    except Exception as e:
        print(f"[å¯è§†åŒ–] ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™: {e}", file=sys.stderr)

    return charts_created


def generate_html_report(query, sql_query, result_df, interpretation, charts):
    """ç”Ÿæˆ HTML æŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>æ•°æ®åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; margin: 40px; background: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 12px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #333; border-bottom: 3px solid #4CAF50; padding-bottom: 10px; }}
        h2 {{ color: #555; margin-top: 30px; }}
        .meta {{ color: #666; font-size: 14px; margin-bottom: 20px; }}
        .query {{ background: #e3f2fd; padding: 15px; border-left: 4px solid #2196F3; border-radius: 4px; margin: 20px 0; }}
        .sql {{ background: #f5f5f5; padding: 15px; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 13px; overflow-x: auto; }}
        .response {{ background: #fff9e6; padding: 20px; border-radius: 8px; margin: 20px 0; line-height: 1.8; }}
        .chart {{ text-align: center; margin: 30px 0; }}
        .chart img {{ max-width: 100%; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th {{ background: #4CAF50; color: white; padding: 12px; text-align: left; font-weight: 600; }}
        td {{ padding: 10px; border-bottom: 1px solid #ddd; }}
        tr:hover {{ background: #f5f5f5; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š</h1>
        <div class="meta">ç”Ÿæˆæ—¶é—´: {timestamp}</div>

        <h2>â“ ä½ çš„é—®é¢˜</h2>
        <div class="query"><strong>{query}</strong></div>

        <h2>ğŸ” ç”Ÿæˆçš„ SQL æŸ¥è¯¢</h2>
        <div class="sql"><code>{sql_query}</code></div>

        <h2>ğŸ“ˆ æŸ¥è¯¢ç»“æœ</h2>
        <table>
            <tr>
                {"".join(f"<th>{col}</th>" for col in result_df.columns)}
            </tr>
"""

    # æ·»åŠ æ•°æ®è¡Œï¼ˆé™åˆ¶å‰50è¡Œï¼‰
    for _, row in result_df.head(50).iterrows():
        html_content += "<tr>"
        for val in row:
            html_content += f"<td>{val}</td>"
        html_content += "</tr>"

    html_content += """
        </table>
"""

    if len(result_df) > 50:
        html_content += f"<p><em>ï¼ˆä»…æ˜¾ç¤ºå‰50è¡Œï¼Œå…± {len(result_df)} è¡Œæ•°æ®ï¼‰</em></p>"

    html_content += f"""
        <h2>ğŸ’¡ AI è§£è¯»</h2>
        <div class="response">{interpretation.replace(chr(10), '<br>')}</div>
"""

    if charts:
        html_content += "<h2>ğŸ“Š å¯è§†åŒ–å›¾è¡¨</h2>"
        for chart_path in charts:
            chart_name = Path(chart_path).name
            # å¤åˆ¶å›¾è¡¨åˆ°è¾“å‡ºç›®å½•ï¼ˆå¦‚æœè¿˜æ²¡åœ¨é‚£é‡Œï¼‰
            html_content += f"""
            <div class="chart">
                <img src="{chart_name}" alt="å›¾è¡¨">
                <p><em>{chart_name}</em></p>
            </div>
            """

    html_content += """
    </div>
</body>
</html>
    """

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    html_path = OUTPUT_DIR / f'analysis_report_{timestamp}.html'

    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    return html_path


def run_analysis(query, api_key, session_data=None):
    """æ‰§è¡Œæ•°æ®åˆ†ææŸ¥è¯¢"""
    try:
        # æ£€æŸ¥ä¼šè¯
        if not session_data or not session_data.get('temp_path'):
            return None, None, None, "æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆä½¿ç”¨ upload å‘½ä»¤ä¸Šä¼ æ–‡ä»¶"

        temp_path = session_data['temp_path']
        columns = session_data['columns']

        print("  ğŸ“‹ æ­£åœ¨åˆ†ææ•°æ®ç»“æ„...")
        data_schema = get_data_schema(temp_path, columns)

        print("  ğŸ¤– ä½¿ç”¨ GLM ç”Ÿæˆ SQL...")
        sql_query, error = generate_sql_with_glm(query, data_schema, api_key)
        if error:
            return None, None, None, error

        print(f"  âœ“ SQL: {sql_query[:100]}...")

        print("  ğŸ“Š æ‰§è¡ŒæŸ¥è¯¢...")
        result_df, error = execute_sql(sql_query, temp_path)
        if error:
            return None, None, None, error

        print(f"  âœ“ è¿”å› {len(result_df)} è¡Œç»“æœ")

        print("  ğŸ’¡ AI è§£è¯»ç»“æœ...")
        interpretation = interpret_results_with_glm(query, result_df, api_key)

        print("  ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–...")
        charts = create_visualization(result_df, query)

        return sql_query, result_df, interpretation, charts, None

    except Exception as e:
        return None, None, None, None, f"åˆ†æå‡ºé”™: {e}"


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    if len(sys.argv) < 3:
        print("="*60)
        print("ğŸ“Š AI æ•°æ®åˆ†æå¸ˆ (GLMç‰ˆæœ¬)")
        print("="*60)
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  ä¸Šä¼ æ–‡ä»¶:")
        print("    python data_analyst_core.py upload <æ–‡ä»¶è·¯å¾„> \"<é—®é¢˜>\" --api-key <GLM_API_KEY>")
        print("\n  ç»§ç»­æé—®:")
        print("    python data_analyst_core.py query \"<é—®é¢˜>\" --api-key <GLM_API_KEY>")
        print("\nç¤ºä¾‹:")
        print("    python data_analyst_core.py upload data.csv \"é”€å”®é¢æœ€é«˜çš„10ä¸ªäº§å“\" --api-key xxx")
        sys.exit(1)

    command = sys.argv[1].lower()
    api_key = None

    # è§£æ API Key
    for i, arg in enumerate(sys.argv):
        if arg == '--api-key' and i + 1 < len(sys.argv):
            api_key = sys.argv[i + 1]
            break

    if not api_key:
        print("âŒ é”™è¯¯: è¯·æä¾› --api-key å‚æ•°")
        print("è·å– GLM API Key: https://open.bigmodel.cn/")
        sys.exit(1)

    # ä¸Šä¼ æ–‡ä»¶å‘½ä»¤
    if command == 'upload':
        if len(sys.argv) < 4:
            print("ç”¨æ³•: python data_analyst_core.py upload <æ–‡ä»¶è·¯å¾„> \"<é—®é¢˜>\" --api-key <API_KEY>")
            sys.exit(1)

        file_path = sys.argv[2]
        query = sys.argv[3]

        print(f"ğŸ“Š æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
        temp_path, columns, df, error = preprocess_file(file_path)

        if error:
            print(f"âŒ {error}")
            sys.exit(1)

        print(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸ")
        print(f"   - æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print(f"   - åˆ—å: {', '.join(columns)}")

        # ä¿å­˜ä¼šè¯
        session_data = save_session(temp_path, columns, df, api_key)
        print(f"ğŸ’¾ ä¼šè¯å·²ä¿å­˜")

        # æ‰§è¡Œåˆ†æ
        print(f"\nğŸ” æ­£åœ¨åˆ†æ: {query}")
        sql_query, result_df, interpretation, charts, error = run_analysis(query, api_key, session_data)

        if error:
            print(f"âŒ {error}")
            sys.exit(1)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“ˆ åˆ†æç»“æœ")
        print("="*60)
        print(f"\n{interpretation}\n")
        print("æ•°æ®è¡¨æ ¼:")
        print(result_df.to_string(index=False))
        print("="*60)

        # ç”ŸæˆæŠ¥å‘Š
        html_path = generate_html_report(query, sql_query, result_df, interpretation, charts)
        print(f"\nğŸ“„ HTML æŠ¥å‘Š: {html_path}")

        if charts:
            print(f"ğŸ“Š å›¾è¡¨æ–‡ä»¶:")
            for chart in charts:
                print(f"   - {chart}")

        print(f"\nğŸ’¡ ç»§ç»­æé—®:")
        print(f"   python {sys.argv[0]} query \"<ä½ çš„é—®é¢˜>\" --api-key {api_key[:10]}...")

    # ç»§ç»­æŸ¥è¯¢å‘½ä»¤
    elif command == 'query':
        if len(sys.argv) < 3:
            print("ç”¨æ³•: python data_analyst_core.py query \"<é—®é¢˜>\" --api-key <API_KEY>")
            sys.exit(1)

        query = sys.argv[2]

        # åŠ è½½ä¼šè¯
        session_data = load_session()
        if not session_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ´»åŠ¨ä¼šè¯ï¼Œè¯·å…ˆä½¿ç”¨ upload å‘½ä»¤ä¸Šä¼ æ–‡ä»¶")
            sys.exit(1)

        print(f"ğŸ” æ­£åœ¨åˆ†æ: {query}")
        sql_query, result_df, interpretation, charts, error = run_analysis(query, api_key, session_data)

        if error:
            print(f"âŒ {error}")
            sys.exit(1)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ“ˆ åˆ†æç»“æœ")
        print("="*60)
        print(f"\n{interpretation}\n")
        print("æ•°æ®è¡¨æ ¼:")
        print(result_df.to_string(index=False))
        print("="*60)

        # ç”ŸæˆæŠ¥å‘Š
        html_path = generate_html_report(query, sql_query, result_df, interpretation, charts)
        print(f"\nğŸ“„ HTML æŠ¥å‘Š: {html_path}")

        if charts:
            print(f"ğŸ“Š å›¾è¡¨æ–‡ä»¶:")
            for chart in charts:
                print(f"   - {chart}")

    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        print("å¯ç”¨å‘½ä»¤: upload, query")
        sys.exit(1)


if __name__ == '__main__':
    main()
